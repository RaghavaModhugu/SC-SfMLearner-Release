{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pdb\n",
    "from collections import defaultdict \n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import animation\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter  \n",
    "import matplotlib.patches as patches\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "import argoverse\n",
    "from argoverse.data_loading.argoverse_tracking_loader import ArgoverseTrackingLoader\n",
    "import argoverse.visualization.visualization_utils as viz_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from imageio import imread, imsave\n",
    "from skimage.transform import resize as imresize\n",
    "#from scipy.misc import imresize\n",
    "import numpy as np\n",
    "from path import Path\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models import DispResNet\n",
    "import models\n",
    "from utils import tensor2array\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pdb\n",
    "\n",
    "from inverse_warp import pose_vec2mat\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESNET_LAYERS = 18\n",
    "\n",
    "if RESNET_LAYERS == 18:\n",
    "    DISPNET_PRETRAINED = './Pretrained_models/SC-SFM-L_/resnet18_depth_256/09-05-23:14/dispnet_model_best.pth.tar'\n",
    "    POSENET_PRETRAINED = './Pretrained_models/SC-SFM-L_/resnet18_depth_256/09-05-23:14/exp_pose_model_best.pth.tar'\n",
    "if RESNET_LAYERS == 50:\n",
    "    DISPNET_PRETRAINED = '/Users/raghavamodhugu/Desktop/IIITH/Trajectory_prediction/argoverse_tracks_generation/SC-SfMLearner-Release/Pretrained_models/SC-SFM-L_/resnet50_depth_256_argo/09-06-13:59/dispnet_model_best.pth.tar'\n",
    "    POSENET_PRETRAINED = '/Users/raghavamodhugu/Desktop/IIITH/Trajectory_prediction/argoverse_tracks_generation/SC-SfMLearner-Release/Pretrained_models/SC-SFM-L_/resnet50_depth_256_argo/09-06-13:59/exp_pose_model_best.pth.tar'\n",
    "\n",
    "ORIGINAL_HEIGHT = 1024\n",
    "ORIGINAL_WIDTH = 1792\n",
    "\n",
    "RESIZE_H = 256\n",
    "RESIZE_W = 448"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_net = DispResNet(RESNET_LAYERS, False).to(device)\n",
    "weights = torch.load(DISPNET_PRETRAINED , map_location=device)\n",
    "disp_net.load_state_dict(weights['state_dict'])\n",
    "disp_net.eval()\n",
    "\n",
    "pose_net = models.PoseResNet(RESNET_LAYERS, False).to(device)\n",
    "weights = torch.load(POSENET_PRETRAINED , map_location=device)\n",
    "pose_net.load_state_dict(weights['state_dict'], strict=False)\n",
    "pose_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resized_img(file, rh=256, rw=448, h=1024, w=1792):\n",
    "    img = imread(file).astype(np.float32)[:h,:w]\n",
    "    img = imresize(img, (rh, rw), anti_aliasing=True).astype(np.float32)\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    tensor_img = torch.from_numpy(img).unsqueeze(0)\n",
    "    return tensor_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "[file.split('_')[0] for file in os.listdir() if file[-4:] == '.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir =  '/Users/raghavamodhugu/Downloads/Argoverse_samples/argoverse-tracking/sample'\n",
    "argoverse_loader = ArgoverseTrackingLoader(root_dir)\n",
    "camera = argoverse_loader.CAMERA_LIST[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_transform_point_cloud(point_cloud, frame, log):\n",
    "    \"\"\"Apply the SE(3) transformation to this point cloud.\n",
    "    Args:\n",
    "        point_cloud: Array of shape (N, 3)\n",
    "    Returns:\n",
    "        transformed_point_cloud: Array of shape (N, 3)\n",
    "    \"\"\"\n",
    "    transform_matrix = pd.read_csv('{}_CameraTrajectory.csv'.format(log), header=None).values[frame].reshape(3,4) \n",
    "    transform_matrix = np.vstack((transform_matrix, np.array([0.0, 0.0, 0.0, 1.0])))\n",
    "    \n",
    "    # convert to homogeneous\n",
    "    num_pts = point_cloud.shape[0]\n",
    "    homogeneous_pts = np.hstack([point_cloud, np.ones((num_pts, 1))])\n",
    "    transformed_point_cloud = homogeneous_pts.dot(transform_matrix.T)\n",
    "    return transformed_point_cloud[:, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pointcloud_from_depthmap(u, v, depthmap, intrinsics):\n",
    "    fx = intrinsics[0][0]\n",
    "    fy = intrinsics[1][1]\n",
    "    cx = intrinsics[0][2]\n",
    "    cy = intrinsics[1][2]\n",
    "    C = np.array([cx, cy])\n",
    "    F = np.array([fx, fy])\n",
    "    h, w = depthmap.shape\n",
    "    z = depthmap[v, u]\n",
    "    x = (u - cx) * z / fx\n",
    "    y = (v - cy) * z / fy\n",
    "    return np.array([u,v,z]), np.array([x, y, z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_3d_point(argoverse_data, idx, camera, matched_point):\n",
    "    img = argoverse_data.get_image_sync(idx,camera = camera)\n",
    "    calib = argoverse_data.get_calibration(camera)\n",
    "    pc = argoverse_data.get_lidar(idx)\n",
    "    uv = calib.project_ego_to_image(pc).T\n",
    "    idx_ = np.where(np.logical_and.reduce((uv[0, :] >= 0.0, uv[0, :] < np.shape(img)[1] - 1.0,\n",
    "                                                      uv[1, :] >= 0.0, uv[1, :] < np.shape(img)[0] - 1.0,\n",
    "                                                      uv[2, :] > 0)))\n",
    "    idx_ = idx_[0]\n",
    "    uv1 =uv[:, idx_]\n",
    "    pc1 =pc[idx_, :]\n",
    "    u = uv1.T\n",
    "    nearest_index = np.argmin(np.sum(np.abs(u[:,:2]-np.array(matched_point)), axis=1))\n",
    "    uvd, point_in_3d = u[nearest_index], pc1[nearest_index]\n",
    "    point_in_3d = argoverse_data.get_pose(idx, log).transform_point_cloud(point_in_3d.reshape(1,-1))\n",
    "    return uvd, point_in_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converter_class(x):\n",
    "    return x[2:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tracks(ob_bbox_path, log, track_file_path, argoverse_data, camera):\n",
    "    ORIGINAL_HEIGHT = 1024\n",
    "    ORIGINAL_WIDTH = 1792\n",
    "\n",
    "    RESIZE_H = 256\n",
    "    RESIZE_W = 448\n",
    "    \n",
    "    object_annotations = pd.read_csv(ob_bbox_path, converters={'class': converter_class})\n",
    "    \n",
    "    num_lidar = len(argoverse_data.get_image_list_sync(camera))\n",
    "    \n",
    "    trackfile = open(track_file_path, \"w\")\n",
    "    header='{},{},{},{},{},{},{},{},{},{},{},{}\\n'.format('log', 'frame', 'trackid', 'class', 'xmin', 'ymin', 'xmax', 'ymax', 'w_x', 'w_y', 'w_z', 'frame_id')\n",
    "    trackfile.writelines(header)\n",
    "    \n",
    "    global_pose = argoverse_data.get_pose(0, log).transform_matrix\n",
    "    intrinsics = argoverse_loader.get_calibration(camera, log).camera_config.intrinsic[:3, :3].astype(np.float32)\n",
    "    \n",
    "    for frame in range(1, num_lidar):\n",
    "        \n",
    "        #Depth\n",
    "        img_path = argoverse_data.get_image_sync(frame, camera, load=False)\n",
    "        #img = argoverse_data.get_image_sync(frame, camera, load=True)\n",
    "        img = imread(img_path).astype(np.float32)[:1024,:1792]\n",
    "        img = imresize(img, (RESIZE_H, RESIZE_W), anti_aliasing=True).astype(np.float32)\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "        tensor_img = torch.from_numpy(img).unsqueeze(0)\n",
    "        tensor_img = ((tensor_img/255 - 0.45)/0.225).to(device)\n",
    "        disp = disp_net(tensor_img)\n",
    "        disp_resized = torch.nn.functional.interpolate(disp,(ORIGINAL_HEIGHT, ORIGINAL_WIDTH), mode=\"bilinear\", align_corners=False)\n",
    "        depthmap = (1/disp_resized).detach().cpu().squeeze().numpy()\n",
    "        \n",
    "        #Pose\n",
    "        file1 = argoverse_data.get_image_sync(frame, camera, load=False)\n",
    "        file0 = argoverse_data.get_image_sync(frame-1, camera, load=False)\n",
    "        img0 = get_resized_img(file0)\n",
    "        img1 = get_resized_img(file1)\n",
    "        tensor_img0 = ((img0/255 - 0.45)/0.225).to(device)\n",
    "        tensor_img1 = ((img1/255 - 0.45)/0.225).to(device)\n",
    "        pose = pose_net(tensor_img0, tensor_img1)\n",
    "        #pose_6d.append(pose.squeeze(0).detach().cpu().numpy())\n",
    "        pose_mat = pose_vec2mat(pose).squeeze(0).detach().cpu().numpy()\n",
    "        pose_mat = np.vstack([pose_mat, np.array([0, 0, 0, 1])])\n",
    "        global_pose = global_pose @  np.linalg.inv(pose_mat)\n",
    "        \n",
    "        #get_3d_point(argoverse_data, frame, camera, matched_point)\n",
    "        f = os.path.join(object_det_dir, img_path.split('/')[-3], img_path.split('/')[-2], img_path.split('/')[-1])\n",
    "        ob_ann = object_annotations[object_annotations.frame == f].values\n",
    "        for ann in ob_ann:\n",
    "            #print(ann)\n",
    "            path_split = ann[1].split('/')\n",
    "            x_s, y_s, w, h = ann[-4:]\n",
    "            img_path = os.path.join(root_dir, path_split[-3], path_split[-2], path_split[-1])\n",
    "            matched_point = x_center, y_center = int(x_s+w/2), int(y_s+h/2)\n",
    "            if (x_center > ORIGINAL_WIDTH) or (y_center > ORIGINAL_HEIGHT): \n",
    "                print(x_center, y_center)\n",
    "                continue\n",
    "            #uvd, point_3d = get_3d_point(argoverse_data, frame, camera, matched_point)\n",
    "            uvd, point_3d = generate_pointcloud_from_depthmap(x_center, y_center, depthmap, intrinsics)\n",
    "            point = np.ones((4, 1))\n",
    "            point[0, 0] = point_3d[0]\n",
    "            point[1, 0] = point_3d[1]\n",
    "            point[2, 0] = point_3d[2]\n",
    "            point_3d = point\n",
    "            point_3d = global_pose@point_3d\n",
    "            ann = np.append(ann, point_3d.reshape(-1)[:3])\n",
    "            ann = np.append(ann, np.array([frame]))\n",
    "            line = '{},{},{},{},{},{},{},{},{},{},{},{}\\n'.format(ann[0],ann[1],ann[2],ann[3],ann[4],ann[5],ann[6],ann[7],ann[8],ann[9],ann[10],ann[11])\n",
    "            trackfile.writelines(line)\n",
    "        x, y, z = argoverse_data.get_pose(frame).translation\n",
    "        line = '{},{},{},{},{},{},{},{},{},{},{},{}\\n'.format(log,ann[1],0,'car',-1,-1,-1,-1,x,y,z,frame)\n",
    "        trackfile.writelines(line)\n",
    "    trackfile.close()\n",
    "    return track_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_trajectories(extracted_trajectories, mincount = 10):\n",
    "    track_ids, counts = np.unique(extracted_trajectories.trackid, return_counts=True)\n",
    "    filtered_extracted_trajectories = extracted_trajectories[extracted_trajectories.trackid.isin(track_ids[counts > mincount])]\n",
    "    trajectory_dataframes = []\n",
    "    for trackid in np.unique(filtered_extracted_trajectories.trackid):\n",
    "        one_track = filtered_extracted_trajectories[filtered_extracted_trajectories.trackid == trackid]\n",
    "        classes, counts = np.unique(one_track['class'], return_counts = True)\n",
    "        max_class_trajectory = one_track[one_track['class']==classes[np.argmax(counts)]]\n",
    "        trajectory_dataframes.append(max_class_trajectory)\n",
    "    return pd.concat(trajectory_dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "forecasting_file_id = 0\n",
    "def make_forecasting_traj_files(forecasting_file_id, filtered_extracted_trajectories):\n",
    "    header='TIMESTAMP,TRACK_ID,OBJECT_TYPE,X,Y,CITY_NAME\\n'\n",
    "    for agentid in np.unique(filtered_extracted_trajectories.trackid):\n",
    "        if agentid == 0:\n",
    "            continue\n",
    "        lines = ''\n",
    "        for trackid in np.unique(filtered_extracted_trajectories.trackid):\n",
    "            one_track = filtered_extracted_trajectories[filtered_extracted_trajectories.trackid == trackid]\n",
    "            classes, counts = np.unique(one_track['class'], return_counts = True)\n",
    "            if trackid == 0:\n",
    "                obj_type = 'AV'\n",
    "            elif agentid == trackid:\n",
    "                obj_type = 'AGENT'\n",
    "            else:\n",
    "                obj_type = 'OTHER'\n",
    "            for record in one_track.values:\n",
    "                lines+='{},{},{},{},{},{}\\n'.format(int(str(timestamps[int(record[-1])])[:10])/10, log+'-'+str(trackid), obj_type, record[-4], record[-3], argoverse_data.city_name)\n",
    "        forecasting_file_id+=1\n",
    "        final_trajectory_file = open('{}.csv'.format(str(forecasting_file_id).zfill(4)), \"w\")\n",
    "        final_trajectory_file.writelines(header)\n",
    "        final_trajectory_file.writelines(lines)\n",
    "        final_trajectory_file.close()\n",
    "    return forecasting_file_id\n",
    "    #print(max_class_trajectory.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "object_det_dir = '/ssd_scratch/cvit/raghava.modhugu/argoverse_tracking/train'\n",
    "st = time.time()\n",
    "forecasting_file_id = 0\n",
    "for log in argoverse_loader.log_list[1:]:\n",
    "    log = '74750688-7475-7475-7475-474752397312'\n",
    "    argoverse_data = argoverse_loader.get(log)\n",
    "    timestamps = list(argoverse_data.timestamp_lidar_dict.keys())\n",
    "    ob_bbox_path = '{}_detection.csv'.format(log)\n",
    "    track_file_path = '{}_tracks_bbox.csv'.format(log)\n",
    "    path = get_tracks(ob_bbox_path, log, track_file_path, argoverse_data, camera)\n",
    "    extracted_trajectories = pd.read_csv(path)\n",
    "    filtered_extracted_trajectories = filter_trajectories(extracted_trajectories)\n",
    "    forecasting_file_id = make_forecasting_traj_files(forecasting_file_id, filtered_extracted_trajectories)\n",
    "    print(forecasting_file_id)\n",
    "    break\n",
    "print(time.time()-st)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
